<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <!-- Primary Meta Tags -->
  <!-- TODO: Replace with your paper title and author names -->
  <meta name="title" content="VO-DP: Semantic-Geometric Adaptive Diffusion Policy for Vision-Only Robotic Manipulation">
  <!-- TODO: Write a compelling 150-160 character description of your research -->
  <meta name="description" content="Semantic-Geometric Adaptive Diffusion Policy for Vision-Only Robotic Manipulation">
  <!-- TODO: Add 5-10 relevant keywords for your research area -->
  <meta name="keywords" content="Representation Learning; Imitation Learning; Perception for Grasping and Manipulation">
  <!-- TODO: List all authors -->
  <meta name="author" content="Zehao Ni, Yonghao He, Lingfeng Qian, Jilei Mao, Fa Fu, Wei Sui, Hu Su, Junran Peng, Zhipeng Wang, Bin He">
  <meta name="robots" content="index, follow">
  <meta name="language" content="English">
  
  <!-- Open Graph / Facebook -->
  <meta property="og:type" content="article">
  <!-- TODO: Replace with your institution or lab name -->
  <meta property="og:site_name" content="D-Robotics">
  <!-- TODO: Same as paper title above -->
  <meta property="og:title" content="VO-DP: Semantic-Geometric Adaptive Diffusion Policy for Vision-Only Robotic Manipulation">
  <!-- TODO: Same as description above -->
  <meta property="og:description" content="Semantic-Geometric Adaptive Diffusion Policy for Vision-Only Robotic Manipulation">
  <!-- TODO: Replace with your actual website URL -->
  <meta property="og:url" content="https://d-robotics-ai-lab.github.io/vodp/">
  <!-- TODO: Create a 1200x630px preview image and update path -->
  <meta property="og:image" content="https://YOUR_DOMAIN.com/static/images/social_preview.png">
  <meta property="og:image:width" content="1200">
  <meta property="og:image:height" content="630">
  <meta property="og:image:alt" content="VO-DP - Research Preview">
  <meta property="article:published_time" content="2025-09-01T00:00:00.000Z">
  <meta property="article:author" content="Zehao Ni">
  <meta property="article:section" content="Research">
  <meta property="article:tag" content="Vision-Only">
  <meta property="article:tag" content="Representation Learning">
  <meta property="article:tag" content="Imitation Learning">
  <meta property="article:tag" content="Perception for Grasping and Manipulation">

  <!-- Twitter -->
  <meta name="twitter:card" content="summary_large_image">
  <!-- TODO: Replace with your lab/institution Twitter handle -->
  <meta name="twitter:site" content="@YOUR_TWITTER_HANDLE">
  <!-- TODO: Replace with first author's Twitter handle -->
  <meta name="twitter:creator" content="@AUTHOR_TWITTER_HANDLE">
  <!-- TODO: Same as paper title above -->
  <meta name="twitter:title" content="PAPER_TITLE">
  <!-- TODO: Same as description above -->
  <meta name="twitter:description" content="BRIEF_DESCRIPTION_OF_YOUR_RESEARCH_CONTRIBUTION_AND_FINDINGS">
  <!-- TODO: Same as social preview image above -->
  <meta name="twitter:image" content="https://YOUR_DOMAIN.com/static/images/social_preview.png">
  <meta name="twitter:image:alt" content="PAPER_TITLE - Research Preview">

  <!-- Academic/Research Specific -->
  <meta name="citation_title" content="VO-DP: Semantic-Geometric Adaptive Diffusion Policy for Vision-Only Robotic Manipulation">
  <meta name="citation_author" content="Ni, Zehao">
  <meta name="citation_author" content="Yonghao, He">
  <meta name="citation_author" content="Lingfeng, Qian">
  <meta name="citation_author" content="Jilei, Mao">
  <meta name="citation_author" content="Fa, Fu">
  <meta name="citation_author" content="Wei, Sui">
  <meta name="citation_author" content="Hu, Su">
  <meta name="citation_author" content="Junran, Peng">
  <meta name="citation_author" content="Zhipeng, Wang">
  <meta name="citation_author" content="Bin, He">
  <meta name="citation_publication_date" content="2025">
  <meta name="citation_conference_title" content="Arxiv">
  <meta name="citation_pdf_url" content="https://YOUR_DOMAIN.com/static/pdfs/paper.pdf">
  
  <!-- Additional SEO -->
  <meta name="theme-color" content="#2563eb">
  <meta name="msapplication-TileColor" content="#2563eb">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="default">
  
  <!-- Preconnect for performance -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link rel="preconnect" href="https://ajax.googleapis.com">
  <link rel="preconnect" href="https://documentcloud.adobe.com">
  <link rel="preconnect" href="https://cdn.jsdelivr.net">


  <!-- TODO: Replace with your paper title and authors -->
  <title>VO-DP</title>
  
  <!-- Favicon and App Icons -->
  <link rel="icon" type="image/x-icon" href="static/images/favicon.png">
  <link rel="apple-touch-icon" href="static/images/favicon.png">
  
  <!-- Critical CSS - Load synchronously -->
  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/index.css">
  
  <!-- Non-critical CSS - Load asynchronously -->
  <link rel="preload" href="static/css/bulma-carousel.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/bulma-slider.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/fontawesome.all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  
  <!-- Fallback for browsers that don't support preload -->
  <noscript>
    <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  </noscript>
  
  <!-- Fonts - Optimized loading -->
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700;800&display=swap" rel="stylesheet">
  
  <!-- Defer non-critical JavaScript -->
  <script defer src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script defer src="static/js/bulma-carousel.min.js"></script>
  <script defer src="static/js/bulma-slider.min.js"></script>
  <script defer src="static/js/index.js"></script>
  
  <!-- Structured Data for Academic Papers -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "ScholarlyArticle",
    "headline": "PAPER_TITLE",
    "description": "BRIEF_DESCRIPTION_OF_YOUR_RESEARCH_CONTRIBUTION_AND_FINDINGS",
    "author": [
      {
        "@type": "Person",
        "name": "FIRST_AUTHOR_NAME",
        "affiliation": {
          "@type": "Organization",
          "name": "INSTITUTION_NAME"
        }
      },
      {
        "@type": "Person",
        "name": "SECOND_AUTHOR_NAME",
        "affiliation": {
          "@type": "Organization",
          "name": "INSTITUTION_NAME"
        }
      }
    ],
    "datePublished": "2024-01-01",
    "publisher": {
      "@type": "Organization",
      "name": "CONFERENCE_OR_JOURNAL_NAME"
    },
    "url": "https://YOUR_DOMAIN.com/YOUR_PROJECT_PAGE",
    "image": "https://YOUR_DOMAIN.com/static/images/social_preview.png",
    "keywords": ["KEYWORD1", "KEYWORD2", "KEYWORD3", "machine learning", "computer vision"],
    "abstract": "FULL_ABSTRACT_TEXT_HERE",
    "citation": "BIBTEX_CITATION_HERE",
    "isAccessibleForFree": true,
    "license": "https://creativecommons.org/licenses/by/4.0/",
    "mainEntity": {
      "@type": "WebPage",
      "@id": "https://YOUR_DOMAIN.com/YOUR_PROJECT_PAGE"
    },
    "about": [
      {
        "@type": "Thing",
        "name": "RESEARCH_AREA_1"
      },
      {
        "@type": "Thing", 
        "name": "RESEARCH_AREA_2"
      }
    ]
  }
  </script>
  
  <!-- Website/Organization Structured Data -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "Organization",
    "name": "INSTITUTION_OR_LAB_NAME",
    "url": "https://YOUR_INSTITUTION_WEBSITE.com",
    "logo": "https://YOUR_DOMAIN.com/static/images/favicon.ico",
    "sameAs": [
      "https://twitter.com/YOUR_TWITTER_HANDLE",
      "https://github.com/YOUR_GITHUB_USERNAME"
    ]
  }
  </script>
</head>
<body>
  <!-- Scroll to Top Button -->
  <button class="scroll-to-top" onclick="scrollToTop()" title="Scroll to top" aria-label="Scroll to top">
    <i class="fas fa-chevron-up"></i>
  </button>

  <!-- More Works Dropdown -->
  <div class="more-works-container">
    <button class="more-works-btn" onclick="toggleMoreWorks()" title="View More Works from Our Lab">
      <i class="fas fa-flask"></i>
      More Works
      <i class="fas fa-chevron-down dropdown-arrow"></i>
    </button>
    <div class="more-works-dropdown" id="moreWorksDropdown">
      <div class="dropdown-header">
        <h4>More Works from Our Lab</h4>
        <button class="close-btn" onclick="toggleMoreWorks()">
          <i class="fas fa-times"></i>
        </button>
      </div>
      <div class="works-list">
        <!-- TODO: Replace with your lab's related works -->
        <a href="https://arxiv.org/abs/2412.14680" class="work-item" target="_blank">
          <div class="work-info">
            <!-- TODO: Replace with actual paper title -->
            <h5>A Light-Weight Framework for Open-Set Object Detection with Decoupled Feature Alignment in Joint Space</h5>
            <!-- TODO: Replace with brief description -->
            <!-- <p>A lightweight framework that enhances real-time performance for robotic systems by using an MLP adaptor to
              decouple and align visual and text features in a joint space.</p> -->
            <!-- TODO: Replace with venue and year -->
            <span class="work-venue">2025 IEEE International Conference on Robotics and Automation (ICRA)</span>
          </div>
          <i class="fas fa-external-link-alt"></i>
        </a>
        <!-- TODO: Add more related works or remove extra items -->
        <a href="https://openaccess.thecvf.com/content/CVPR2025W/MEIS/papers/Yu_Efficient_Task-specific_Conditional_Diffusion_Policies_Shortcut_Model_Acceleration_and_SO3_CVPRW_2025_paper.pdf" class="work-item" target="_blank">
          <div class="work-info">
            <h5>Efficient Task-Specific Conditional Diffusion Policies: Shortcut Model Acceleration and SO(3) Optimization</h5>
            <!-- <p>A Classifier-Free Shortcut Diffusion Policy (CF-SDP) that significantly accelerates robot action generation
              by integrating shortcut-based sampling and SO(3) manifold optimization. </p> -->
            <span class="work-venue">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops, 2025</span>
          </div>
          <i class="fas fa-external-link-alt"></i>
        </a>
        <!-- <a href="https://arxiv.org/abs/PAPER_ID_3" class="work-item" target="_blank">
          <div class="work-info">
            <h5>Paper Title 3</h5>
            <p>Brief description of the work and its main contribution.</p>
            <span class="work-venue">Conference/Journal 2023</span>
          </div>
          <i class="fas fa-external-link-alt"></i>
        </a> -->
      </div>
    </div>
  </div>

  <main id="main-content">
  <section class="hero">
    <div class="hero-body" style="padding-bottom: 0px">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <!-- TODO: Replace with your paper title -->
            <h1 class="title is-1 publication-title" style="font-size: 34px;">
              <font color="#eb4e27">VO-DP</font>: Semantic-Geometric Adaptive 
              <font color="#eb4e27">D</font>iffusion <font color="#eb4e27">P</font>olicy for 
              <font color="#eb4e27">V</font>ision-<font color="#eb4e27">O</font>nly Robotic Manipulation
            </h1>

            <div class="is-size-5 publication-authors">
              <!-- TODO: Replace with your paper authors and their personal links -->
              <span class="author-block">
                <a href="FIRST AUTHOR PERSONAL LINK" target="_blank">Zehao Ni</a><sup>1,2,5,6*</sup>,
              </span>
              <span class="author-block">
                <a href="SECOND AUTHOR PERSONAL LINK" target="_blank">Yonghao He</a><sup>1*†</sup>,
              </span>
              <span class="author-block">
                <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Lingfeng Qian</a><sup>1*</sup>,
              </span>
              <span class="author-block">
                <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Jilei Mao</a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Fa Fu</a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Wei Sui</a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Hu Su</a><sup>4</sup>,
              </span>
              <span class="author-block">
                <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Junran Peng</a><sup>3✉</sup>,
              </span>
              <span class="author-block">
                <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Zhipeng Wang</a><sup>2,5,6</sup>,
              </span>
              <span class="author-block">
                <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Bin He</a><sup>2,5,6✉</sup>
              </span>
            </div>

            <div class="is-size-5 publication-authors" style="margin-bottom: 0px;">
              <!-- TODO: Replace with your institution and conference/journal info -->
              <span class="author-block"  style="font-size: 14px;">
                <sup>1</sup>D-Robotics<br>
                <sup>2</sup>National Key Laboratory of Autonomous Intelligent Unmanned Systems<br>
                <sup>3</sup>University of Science and Technology Beijing<br>
                <sup>4</sup>State Key Laboratory of Multimodal Artificial Intelligence System (MAIS) Institute of Automation of Chinese Academy of Sciences<br>
                <sup>5</sup>Frontiers Science Center for Intelligent Autonomous Systems<br>
                <sup>6</sup>Shanghai Institute of Intelligent Science and Technology, Tongji University<br>
                
              </span>
              <!-- <span class="author-block">
                ICRA 2026
              </span> -->
              <span class="author-block" style="font-size: 16px;">
                <sup>*</sup>Equal Contribution <sup>*</sup>Project Leader <sup>✉</sup>Corresponding authors
              </span>
              <!-- TODO: Remove this line if no equal contribution -->
              <!-- <span class="eql-cntrb"><small><sup>*</sup>Equal Contribution <sup>*</sup>Project Leader <sup>✉</sup>Corresponding authors</small></span> -->
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">

                <!-- TODO: Update with your arXiv paper ID -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2510.15530" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>

                <!-- TODO: Update with your arXiv paper ID -->
                <span class="link-block">
                  <a href="https://arxiv.org/pdf/2510.15530" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>PDF</span>
                  </a>
                </span>

                <!-- TODO: Add your supplementary material PDF or remove this section -->
                <span class="link-block">
                  <a href="https://huggingface.co/datasets/D-Robotics/DRRM" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-database"></i>
                    </span>
                    <span>Dataset</span>
                  </a>
                </span>

                <!-- TODO: Replace with your GitHub repository URL -->
                <span class="link-block">
                  <a href="https://github.com/D-Robotics-AI-Lab/DRRM" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>
              
              </div>
            </div>

          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- Teaser video-->
  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <center>
          <!-- static/images/vodp-all.jpg -->
          <img src="static/images/compare.jpg" 
            alt="VO-DP" class="blend-img-background center-image" 
            style="max-width: 90%; height: auto;" loading="lazy">
        </center>
        <p>
            VO-DP is a vision-only method for visuomotor
            robotic manipulation: it takes single-view RGB images as
            input, uses large vision models to extract semantic and geometric
            features from observations, and provides high-quality
            conditional inputs for the policy head. Experiments show
            it matches point cloud-based DP3’s accuracy in simulation,
            outperforms it significantly in real-world tasks, and notably
            boosts vision-only method accuracy.
        </p>
      </div>
    </div>
  </section>
  <!-- End teaser video -->

  <!-- Paper abstract -->
  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified" style="width: 100%; font-size: 16px;">
            <!-- TODO: Replace with your paper abstract -->
            <p>
              In the context of imitation learning, visuomotor-based
              diffusion policy learning is one of the main directions in
              robotic manipulation. Most of these approaches rely on point
              clouds as observation inputs and construct scene representations
              through point clouds feature learning, which enables
              them to achieve remarkable accuracy. However, the existing
              literature lacks an in-depth exploration of vision-only solutions
              that have significant potential. In this paper, we propose a
              Vision-Only and single-view Diffusion Policy learning method
              (VO-DP) that leverages pretrained visual foundation models
              to achieve effective fusion of semantic and geometric features.
              We utilize intermediate features from VGGT incorporating
              semantic features from DINOv2 and geometric features from
              Alternating Attention blocks. Features are fused via cross-attention
              and spatially compressed with a CNN to form the
              input to the policy head. Extensive experiments demonstrate
              that VO-DP not only outperforms the vision-only baseline
              DP significantly but also exhibits distinct performance trends
              against the point cloud-based method DP3: in simulation tasks,
              VO-DP achieves an average success rate of 64.6%—on par with
              DP3 64.0% and far higher than DP 34.8%, while in real-world
              tasks, it reaches 87.9%, outperforming both DP3 67.5% and
              DP 11.2% by a notable margin. Further robustness evaluations
              confirm that VO-DP remains highly stable under varying conditions
              including color, size, background, and lighting. Lastly,
              we open-source DRRM (D-Robotics Robotic Manipulation), a
              training library for robotic manipulation. Built on Accelerate,
              this library supports multi-machine and multi-GPU parallel
              training, as well as mixed precision training (e.g., bf16, fp16).
              It is compatible with visuomotor policies such as DP and DP3,
              and also supports the RoboTwin simulator. VO-DP is integrated
              into DRRM. We refer to the project page for the code and videos.
          </div>
        </div>
      </div>
    </div>
    <div class="container is-max-desktop">
      <video poster="" id="tree" autoplay controls muted loop height="100%" preload="metadata">
        <!-- TODO: Add your video file path here -->
        <source src="https://drobotics-ailab.tos-cn-beijing.volces.com/public/vodp/VO-DP-1080_30.mp4" type="video/mp4">
      </video>
    </div>
  </section>
  <!-- End paper abstract -->

  <!-- Method -->
  <section class="section hero">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths" style="width: 100%">
          <h2 class="title is-3">Method</h2>
          <div class="content has-text-justified" style="font-size: 16px;">
            <p>
              Our method VO-DP has four core modules: 1) VGGT Encoder extracts semantic features from
              patchified images via DINOv2 and generates geometric features through its AA network; 2) Semantic-Geometric Fuser
              fuses per-frame geometric and semantic features using residual cross-attention and an FFN; 3) Spatial Compression module
              reshapes fused features, downsamples them with a lightweight ResNet, and concatenates the compressed spatial features
              with proprioceptive observations to form compact scenario representations; 4) Vision-Only Conditioned Action Generation
              module employs a DDPM-based policy head to generate actions using the scenario representations.
            </p>
            <center>
              <img src="static/images/framework_a.jpg" alt="VO-DP" 
              class="blend-img-background center-image" style="max-width: 90%; height: auto;" loading="lazy">
            </center>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End Method -->

  <!-- Effectiveness -->
  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths" style="width: 100%">
          <h2 class="title is-3">Effectiveness</h2>
          <div class="content has-text-justified" style="font-size: 16px;">
            <p>
              VO-DP achieves state-of-the-art (SOTA) performance
              in both simulation and real-world environments,
              while also demonstrating high data efficiency and strong generalization.
              In simulation, it is rigorously evaluated on the challenging 14-task RoboTwin benchmark.
              For real-world tasks such as pick-and-place and stacking, VO-DP is trained with only
              200 demonstrations per task, showcasing exceptional data efficiency. More importantly,
              the method exhibits remarkable generalization capabilities across significant variations,
              maintaining robust performance in rigorous tests for size, appearance, illumination,
              and background robustness, with success rates such as 65.0% on unseen object sizes and 83.3%
              under dynamic lighting interference.
            </p>
            <center>
              <img src="static/images/result.jpg" alt="VO-DP" 
              class="blend-img-background center-image" style="max-width: 90%; height: auto;" loading="lazy">
            </center>
        </div>
      </div>
    </div>
  </section>
  <!-- End Effectiveness -->

  <!-- Simulation evaluation -->
  <section class="section hero">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths" style="width: 100%">
          <h2 class="title is-3">Geometry-Aware Tokens<br>Visualization on Robotwin</h2>
          <div class="content has-text-justified" style="font-size: 16px;">
            <p>
              VO-DP demonstrates its effective spatial
              representation in embodied scenarios through its geometry-aware tokens,
              as evaluated on the challenging RoboTwin benchmark.
              It successfully addresses 14 bimanual tasks by comprehending semantic
              intent and geometric structure from RGB image inputs.
              The policy achieves high success rates by satisfying precise pose constraints
              and maintaining collision-free trajectories across extensive test scenes.
            </p>
            <video poster="" id="tree" autoplay muted loop height="80%" preload="metadata">
              <!-- TODO: Add your video file path here -->
              <source src="https://drobotics-ailab.tos-cn-beijing.volces.com/public/vodp/vggt-vis_30.mp4" type="video/mp4">
            </video>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End Simulation evaluation -->

  <!-- Real-world evaluation -->
  <!-- <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths" style="width: 100%">
          <h2 class="title is-3">Real-world tasks</h2>
          <div class="content has-text-justified" style="font-size: 16px;">
            <p>
              VO-DP is a vision-only method for visuomotor
              robotic manipulation: it takes single-view RGB images as
              input, uses large vision models to extract semantic and geometric
              features from observations, and provides high-quality
              conditional inputs for the policy head. Experiments show
              it matches point cloud-based DP3’s accuracy in simulation,
              outperforms it significantly in real-world tasks, and notably
              boosts vision-only method accuracy.
            </p>
            <p><img src="static/images/vodp-all.jpg" alt="VO-DP" class="blend-img-background center-image" style="max-width: 100%; height: auto;" loading="lazy"></p>
          </div>
        </div>
      </div>
    </div>
  </section> -->
  <!-- End Real-world evaluation -->

  <!-- Robustness evaluation -->
  <!-- <section class="section hero">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths" style="width: 100%">
          <h2 class="title is-3">Robustness evaluation</h2>
          <div class="content has-text-justified" style="font-size: 16px;">
            <p>
              VO-DP is a vision-only method for visuomotor
              robotic manipulation: it takes single-view RGB images as
              input, uses large vision models to extract semantic and geometric
              features from observations, and provides high-quality
              conditional inputs for the policy head. Experiments show
              it matches point cloud-based DP3’s accuracy in simulation,
              outperforms it significantly in real-world tasks, and notably
              boosts vision-only method accuracy.
            </p>
            <p><img src="static/images/vodp-all.jpg" alt="VO-DP" class="blend-img-background center-image" style="max-width: 100%; height: auto;" loading="lazy"></p>
          </div>
        </div>
      </div>
    </div>
  </section> -->
  <!-- End Robustness evaluation -->

  <!--BibTex citation -->
    <section class="section" id="BibTeX">
      <div class="container is-max-desktop content">
        <div class="bibtex-header">
          <h2 class="title">BibTeX</h2>
          <button class="copy-bibtex-btn" onclick="copyBibTeX()" title="Copy BibTeX to clipboard">
            <i class="fas fa-copy"></i>
            <span class="copy-text">Copy</span>
          </button>
        </div>
        <pre id="bibtex-code"><code>@article{ni2025vodp,
          title={VO-DP: Semantic-Geometric Adaptive Diffusion Policy for Vision-Only Robotic Manipulation},
          author={Zehao Ni and Yonghao He and Lingfeng Qian and Jilei Mao and Fa Fu and Wei Sui and Hu Su and Junran Peng and Zhipeng Wang and Bin He},
          journal={arXiv preprint arXiv:2510.15530},
          year={2025}
        }</code></pre>
      </div>
  </section>
  <!--End BibTex citation -->


  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">

            <p>
              This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
              You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
              Commons Attribution-ShareAlike 4.0 International License</a>.
            </p>

          </div>
        </div>
      </div>
    </div>
  </footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

</body>
</html>
